{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indice Invertido",
      "provenance": [],
      "collapsed_sections": [
        "MZBdsjheVyLz",
        "bMCJJHxXztix",
        "HS6h1RvQDQSI",
        "CVnwnXcOfgzR",
        "ylocAc5IXNdp",
        "idvUOM4_8DFP",
        "O7bHLQnRN8kI",
        "hafnYeAVaFaY",
        "8EtIH1Xaa57q",
        "U4ytT4nablzE",
        "cRusxMKucQi5"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmlins/recuperacaoDeInformacao/blob/master/Indice_Invertido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnfWuI5acd8B",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAIV006BGuhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests as rq\n",
        "import re\n",
        "from bs4 import UnicodeDammit\n",
        "import glob\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQjNbC-gQogk",
        "colab_type": "code",
        "outputId": "21032874-1fe9-4ae2-d57d-2b4c8ae722d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZBdsjheVyLz",
        "colab_type": "text"
      },
      "source": [
        "## Funções auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQiuKEzpUCHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Create DataFrame\n",
        "\n",
        "def create_df(info_book_list):\n",
        "  cols = ['Título', 'Autor', 'Nº Páginas', 'Editora', 'Idioma', 'Acabamento', 'ISBN', 'Documento']\n",
        "\n",
        "  book_pd = pd.DataFrame([info_book_list],\\\n",
        "                        columns=cols)\n",
        "\n",
        "  return (book_pd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzgWWQPPwgq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def uniques_el(lista):\n",
        "    l = []\n",
        "    for i in lista:\n",
        "        if i not in l:\n",
        "            l.append(i)\n",
        "    l.sort()\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCQayc2mCTAT",
        "colab_type": "text"
      },
      "source": [
        "## Sites Livros\n",
        "\n",
        "1.   https://www.amazon.com.br/\n",
        "2.   https://www.saraiva.com.br/\n",
        "3.   https://www.travessa.com.br/\n",
        "4.   https://www.traca.com.br/\n",
        "5.   https://www.ciadoslivros.com.br/\n",
        "6.   https://www.extra.com.br/livros/\n",
        "7.   https://www.livrariacultura.com.br\n",
        "8.   https://www.disal.com.br/\n",
        "9.   https://www.americanas.com.br/\n",
        "10.  https://www.submarino.com.br/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMCJJHxXztix",
        "colab_type": "text"
      },
      "source": [
        "# Primeira parte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS6h1RvQDQSI",
        "colab_type": "text"
      },
      "source": [
        "## URLS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8agT_oB3Ovt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "amazon_urls = [ r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/1808.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Box - As Melhores Aventuras Do Sítio Do Picapau Amarelo Monteiro Lobato Amazon.com.br.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Cotoco. O Diário De Um Garoto De 13 Anos - Livros na Amazon Brasil- 9788598078854.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Doctor Who, Neil Gaiman e vários - Livros na Amazon.com.br.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Freud (1900) A interpretação dos sonhos Obras completas volume 4 - 9788535932218 - Livros na Amazon Brasil.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Good Omens Belas Maldições - Livros na Amazon Brasil- 9788528624021.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Os Mitos de Cthulhu - Volume Único (Exclusivo Amazon) Livros.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Redes de Computadores e a Internet Uma Abordagem Top-Down - 9788581436777 - Livros na Amazon Brasil.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Amazon/Positivas/Sobotta - Atlas de Anatomia Humana - 3 Volumes - 9788527732376 - Livros na Amazon Brasil.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4QFc9ayz6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saraiva_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/Alice No País Das Maravilhas (Classic Edition) - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/Steve Jobs - A Biografia - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/Vade Mecum Compacto - Brochura - 21a Ed. 2019 - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/It - A Coisa - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/Como Fazer Amigos e Influenciar Pessoas - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/O Guia do Mochileiro Das Galáxias - Série o Mochileiro Das Galáxias - Vol. 1 - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/O Livrinho do Bebê - Animais, Palavras, Contrários, Cores e Formas - Saraiva.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Saraiva/Positivas/Star Wars - O Arquivo Rebelde - Saraiva.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnyRvccFE9sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "travessa_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/A COR DA MAGIA - Terry Pratchett - Livro.html',                \n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/CYNDI MINHA HISTORIA - Cyndi Lauper Jancee Dunn - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/D. PEDRO II A HISTORIA NAO CONTADA - Paulo Rezzutti - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/JAPONES PARA LEIGOS - Eriko Sato - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/LONELY PLANET ARGENTINA (INCLUI URUGUAI) - Globo Livros - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/MADELINE FINN E OS CÃES DO ABRIGO - Lisa Papp - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/O PODER DA AUTORRESPONSABILIDADE A FERRAMENTA COMPROVADA QUE GERA ALTA PERFORMANCE E RESULTADOS EM POUCO TEMPO - Paulo Vieira - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/CRONICAS PARA LER EM QUALQUER LUGAR - Vários (ver informações no detalhe) - Livro.html',                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/SO PARA UM ALIMENTAÇAO SAUDAVEL PARA QUEM MORA SOZINHO - Rita Lobo - Livro.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Travessa/Positivas/VIDAS SECAS - 140aED.(2019) - Graciliano Ramos - Livro.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUtv4FCmAbza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traca_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/A Casa Dos Budas Ditosos - João Ubaldo Ribeiro - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Cavaleiros Do Zodíaco Vol 1 - Masami Kurumada - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Cem Anos De Solidão - Gabriel Garcia Marquez - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Crianças Famosas  Schubert - Ann Rachlin E Susan Hellard - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Indiozinhos - Mônica Haibara - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Livro Da Esperança - Francisco Cândido Xavier - Emmanuel - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Mozart Don Giovanni - A Vida De Mozart - Lorenzo Da Ponte - Henry Stendhal - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/O Colapso Do Universo - Isaac Asimov - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Políticas Territoriais Na Amazônia - Neli Aparecida De Mello - Traça Livraria e Sebo.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Traça/Positivas/Relações Exteriores Do Brasil Contemporâneo - Danielly Silva Ramos Becard - Traça Livraria e Sebo.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDzRkSA2x6d-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cia_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Afirmar-se com Nietzsche - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Audácia Dessa Mulher, A - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Avaliação - Mito e Desafio - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Breve História do Feminismo no Contexto Euro-americano, Uma - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Corpo Fala, O - A Linguagem Silenciosa da Comunicação Não Verbal - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Cura do Ciúme, A - Aprenda a Confiar, Supere a Possessividade e Salve Seu Relacionamento - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Dom Casmurro - Coleção Grandes Mestres da Literatura Brasileira - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Em Busca do Texto Perfeito - Questões Contemporâneas de Edição, Preparação e Revisão Textual - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Manual do Arquiteto Descalço - Capa Dura - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Feminismo É Feminino?, O - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Gramática de Espanhol Para Brasileiros - Cia. dos Livros.html',\n",
        "            r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Cia dos Livros/Positivas/Livro Nem Só De Ciência Se Faz A Cura - O Que Os Pacientes Me Ensinaram - Cia. dos Livros.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAGWyM7I0LB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extra_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Aromaterapia e as Emoções - Como Usar Óleos Essenciais Para Equilibrar o Corpo e a Mente - Especialidades Médicas no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Once Upon A Time - Irmãos Grimm - Juvenil no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Os Miseráveis - Walcyr Carrasco - Juvenil no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Panelinha - Receitas Que Funcionam - Rita Lobo - Culinária no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - A Pequena Filosofia da Mafalda - Vamos à Sopa - Joaquim Salvador Lavado (Quino) - Infantis no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - A Seleção_ 35 Garotas e uma Coroa - Volume 1 - Kiera Cass - Juvenil no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Como Eu Era Antes de Você - Jojo Moyes - Romance no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Elas Podem... e Devem - o Livro que vai Mexer com Você - Flaviane Brandemberg - Sexologia no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Em Busca de Sentido - Um Psicólogo no Campo de Concentração - Psicologia no Extra.com.br.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Extra/Positivas/Livro - Eu, Fernando Pessoa - em Quadrinhos - Susana Ventura - Quadrinhos Adultos no Extra.com.br.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PQANn-WYwlBE",
        "colab": {}
      },
      "source": [
        "cultura_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro - MAIS ESPERTO QUE O DIABO - Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro - O CAIBALION - Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro - O PODER DO HÁBITO - Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro - QUEM PENSA ENRIQUECE - Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro - SOCIEDADE DO CANSAÇO - Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro GESTALT-TERAPIA - FUNDAMENTOS EPISTEMOLOGICOS | Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro INTRODUÇAO A ALGEBRA LINEAR COM APLICAÇOES | Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro O DIÁRIO DE ANNE FRANK | Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro O MUNDO SEGUNDO FELIPE NETO | Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro O PEQUENO PRINCIPE (LIVRO DE BOLSO) | Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro PRÓLOGO, ATO, EPÍLOGO | Livraria Cultura.html',\n",
        "                r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Livaria Cultura/Positivas/Livro QUEM PENSA ENRIQUECE | Livraria Cultura.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XrpdjP9CBU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "disal_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Ambiente E Os Processos De Maturacao, O - 9788573074567.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Autorretrato E Outras Crônicas - 9788501112330.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Aventuras Na Netoland Com Luccas Neto - 9788555461002.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Exames Laboratoriais E Diagnosticos Em Enfermagem - 9o Ed - 9788527726634.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Fisiologia Endocrina (lange) - 4o Ed - 9788580553918.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Gestao Hospitalar - Para Uma Administracao Eficaz - 9788527733298.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Gramatica Da Lingua Portuguesa Para Leigos - 9788550803395.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Introducao A Analise Matematica - 9788521201687.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Michaelis Minidicionario Alemao - Alemao-portugues - Portugues-alemao - 9788506078563.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Michaelis Minidicionario Espanhol - Espanhol-portugues _ Portugues-espanhol - 9788506078532.html',\n",
        "              r'/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/Disal/Positivas/Michaelis Minidicionario Frances - Frances-portugues _ Portugues-frances - 3a Ed - 9788506078549.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I7Rm0mGoTP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "americanas_urls = [r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - A Cinco Passos de Você nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - A Corrente nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - As 13 Maldições - Edição Econômica nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - Crônicas Saxônicas: A Guerra do Lobo nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - Eu Sou Eric Zimmerman nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - Herdeiro de Sevenwaters - Coleção Sevenwaters - Vol. 4 nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - Historia Da Filosofia Crista nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - Medicina dos Horrores nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - O Cadete e o Capitão nas Lojas Americanas.com.html',\n",
        "                   r'/content/drive/My Drive/9periodo/Recuperação de Informação/Americanas/Positivas/Livro - Paixão Libertadora - Desejo Proibido - Vol. 2 nas Lojas Americanas.com.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkzi1ZKwrGZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submarino_urls = [  r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/LIvro - Luccas Neto em Os Aventureiros no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - A Dama Mais Apaixonada no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - O Labirinto do Fauno no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - Sandman - Edição Definitiva no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - O Rei Leão Livro Oficial com a História do Filme no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - Akira no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - Destinos Divididos no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - Melhor que a Encomenda no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - Um Carinho na Alma + 3 Cordéis do Autor no Submarino.com.html',\n",
        "                    r'/content/drive/My Drive/9periodo/Recuperação de Informação/Submarino/Positivas/Livro - Seja Foda! no Submarino.com.html']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LHEsJ8sEUIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "urls = amazon_urls + saraiva_urls + travessa_urls + traca_urls + cia_urls + extra_urls + cultura_urls + disal_urls + americanas_urls + submarino_urls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPhFcFSDqYz",
        "colab_type": "text"
      },
      "source": [
        "## 1. AMAZON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ-dhdGq7V8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def amazon_single(site):\n",
        "\n",
        "  soup = bs(open(site),\"html.parser\")\n",
        "\n",
        "  book = soup.find('div', {'class':'content'}).find_all('li')[:5]\n",
        "  \n",
        "  infos = soup.find_all('ul')\n",
        "  infos = [info.text.split('\\n') for info in infos]\n",
        "  infos = [info for info in infos]\n",
        "  infos = [info[i].split(':') for info in infos for i in range(len(info))]\n",
        "\n",
        "\n",
        "  infos2 = soup.find('h1', {'id':'title'}).find_all('span')\n",
        "  infos2 = [info.text.split('\\n') for info in infos2]\n",
        "  infos2 = [info for info in infos2]\n",
        "\n",
        "  n_pages = book[0].contents[1].strip().split(' ')[0]\n",
        "  \n",
        "  publishing = book[1].contents[1].strip().split(';')[0]\n",
        "  \n",
        "  language = book[2].contents[1].strip()\n",
        "  \n",
        "  isbn = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'ISBN' or info[i] == 'ISBN 13' or info[i] == 'ISBN-13']\n",
        "  \n",
        "  title = infos2[0][0]\n",
        "\n",
        "  language = infos2[1][0]\n",
        "\n",
        "  binding = infos2[2][0]\n",
        "\n",
        "  author = (soup.find('span', {'class':'author notFaded'})).find('a',{'class':'a-link-normal'}).text\n",
        "\n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "\n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dFiM9BR44A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in amazon_urls:\n",
        "  frame = amazon_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_ama = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d_EgX6d1Qfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_ama"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTCEiDOGyxxe",
        "colab_type": "text"
      },
      "source": [
        "## 2. SARAIVA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlBDXliJ6Mrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saraiva_single(site):\n",
        "  soup = bs(open(site),\"html.parser\")\n",
        "\n",
        "  infos = soup.find('table')\n",
        "  infos = [(re.sub('\\n|<.*?>|\\\\n', '///', str(item))) for item in infos]\n",
        "  infos = [item.split('///') for item in infos]\n",
        "  infos = [elem for item in infos for elem in item if elem !=  '']\n",
        "\n",
        "  isbn = [infos[i+1] for i in range(len(infos)) if infos[i] == 'ISBN' or infos[i] == 'ISBN 13' or infos[i] == 'ISBN-13']\n",
        "\n",
        "  language = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Idioma' or infos[i] == 'Idioma ']\n",
        "\n",
        "  n_pages = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Páginas' or infos[i] == 'Número de páginas' or infos[i] == 'Número de Páginas']\n",
        "\n",
        "  binding = [infos[i+1] for info in infos for i in range(len(infos)) if infos[i] == 'Acabamento' or infos[i] == 'Encadernação' or infos[i] == 'Tipo de produto ' or infos[i] == 'Formato']\n",
        "\n",
        "  author = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Autor']\n",
        "\n",
        "  publishing = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Editora']\n",
        "\n",
        "  title = soup.find('div', {'class':'product-review'}).h1.text\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQs4-Ipo65JC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in saraiva_urls:\n",
        "  frame = saraiva_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_sar = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsDFencN7qNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_sar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2juZY3o-m5l",
        "colab_type": "text"
      },
      "source": [
        "##3. LIVRARIA DA TRAVESSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu_qaodwB8Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def travessa_single(site):\n",
        "  soup = bs(open(url), 'html.parser')\n",
        "  \n",
        "  infos = soup.find('div', {'id':'divDados'}).contents\n",
        "  infos = [(re.sub('\\n|<.*?>|:', '', str(info))) for info in infos]\n",
        "  infos = [info for info in infos if info != '']\n",
        "  infos\n",
        "  \n",
        "  title = [infos[i+1] for i in range(len(infos)) if infos[i] == 'título']\n",
        "\n",
        "  author = [re.sub('autor |tradutor','  ',(infos[i])) for i in range(len(infos)) if re.match('autor|tradutor',infos[i])]\n",
        "\n",
        "  n_pages = [infos[i+1] for i in range(len(infos)) if infos[i] == 'páginas']\n",
        "\n",
        "  binding = [infos[i+1] for i in range(len(infos)) if infos[i] == 'encadernação']\n",
        "\n",
        "  language = [infos[i+1] for i in range(len(infos)) if infos[i] == 'idioma']\n",
        "\n",
        "  isbn = [infos[i+1] for i in range(len(infos)) if infos[i] == 'isbn']\n",
        "\n",
        "  publishing = soup.find('span', {'id': 'lblNomProdutor'}).text.split(':')[1]\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwBO74NnOwyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in travessa_urls:\n",
        "  frame = travessa_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_trav = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYgKHQoAPj2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_trav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RGNLy-nPJby",
        "colab_type": "text"
      },
      "source": [
        "##4. TRAÇA LIVRARIA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxFd85QX3Z45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tracal_single(site):\n",
        "  soup = bs(open(site),\"html.parser\")\n",
        "\n",
        "  infos = soup.find_all('ul')\n",
        "  infos = [info.text.split('\\n') for info in infos]\n",
        "  infos = [info for info in infos]\n",
        "  infos = [info[i].split(':') for info in infos for i in range(len(info))]\n",
        "\n",
        "  n_pages = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Páginas' or info[i] == 'Número de páginas']\n",
        "\n",
        "  binding = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Encadernação' or info[i] == 'Tipo de produto ' or info[i] == 'Formato']\n",
        "\n",
        "  isbn = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'ISBN' or info[i] == 'ISBN 13' or info[i] == 'ISBN-13']\n",
        "\n",
        "  infos2 = soup.find('div', {'class':'box-principal-dados'})\n",
        "  infos2 = [info for info in infos2]\n",
        "  infos2 = [re.sub('\\n|<.*?>','', str(info)).split(\":\") for info in infos2]\n",
        "  infos2 = [info[i].strip() for info in infos2 for i in range(len(info)) if info[i] != '']\n",
        "\n",
        "  language = soup.find('div', {'style':'font-size:18px;line-height:20px;'}).text\n",
        "  language = re.sub('\\n|[L][i][v][r][o][ ][e][m] | (.*?)\\s', '', language)\n",
        "\n",
        "  publishing =infos2[7]\n",
        "\n",
        "  title = soup.find('div', {'class':'box-principal-dados'}).h1.text\n",
        "\n",
        "  author = soup.find('div', {'class':'box-principal-dados'}).a.text  \n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhxamejyL0-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in traca_urls:\n",
        "  frame = tracal_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_traca = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBoXg3P2AW88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_traca"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s5-RsrjsHFa",
        "colab_type": "text"
      },
      "source": [
        "## 5. CIA DOS LIVROS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut4d7ydOxNNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cia_dos_livros_single(site):\n",
        "  soup = bs(open(site), 'html.parser')\n",
        "\n",
        "  title = soup.find('div', {'class':'detailProduct'}).h1.text\n",
        "\n",
        "  author = soup.find('dl', {'class':'autor-editora'}).dd.contents[0].strip()\n",
        "\n",
        "  infos = soup.find('dl', {'id':'bookData'})\n",
        "  infos_list = [info for info in infos]\n",
        "  infos_list = [re.sub('\\n|<.*?>|:|\\t', '', str(info)) for info in infos_list]\n",
        "  infos_list = [item for item in infos_list if item != '']\n",
        "  \n",
        "  language = [infos_list[i+1] for i in range(len(infos_list)) if infos_list[i] == 'Idioma'][0]\n",
        "\n",
        "  n_pages = [infos_list[i+1] for i in range(len(infos_list)) if infos_list[i] == 'Número de páginas'][0]\n",
        "\n",
        "  binding = [infos_list[i+1] for i in range(len(infos_list)) if infos_list[i] == 'Encadernação'][0]\n",
        "\n",
        "  publishing = [infos_list[i+1] for i in range(len(infos_list)) if infos_list[i] == 'Marca'][0]\n",
        "\n",
        "  isbn = [infos_list[i+1] for i in range(len(infos_list)) if infos_list[i] == 'ISBN' or infos_list[i] == 'ISBN 13' or infos_list[i] == 'ISBN-13' or infos_list[i] == 'ISBN13'][0]\n",
        "\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GYJSIJO1kop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in cia_urls:\n",
        "  frame = cia_dos_livros_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_cia = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH5zSrz5_bfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_cia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPi4_ILS1mwH",
        "colab_type": "text"
      },
      "source": [
        "##6. EXTRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOHAK0d_GWsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extra_single(site):\n",
        "  soup = bs(open(site), 'html.parser')\n",
        "  \n",
        "  title = soup.find('div', {'class':'produtoNome'}).h1.b.text\n",
        "\n",
        "  author = soup.find('dl', {'class':'Autor'}).contents\n",
        "  author = [re.sub('\\n|<.*?>', '', str(item)) for item in author]\n",
        "  author = [item.strip() for item in author if item != '']\n",
        "  author = [author[i+1] for i in range(len(author)) if author[i] == 'Autor'][0]\n",
        "\n",
        "  infos = soup.find_all('div', {'class':'wrp'})\n",
        "  infos = [(re.sub('\\n|<.*?>|\\\\n', '', str(item))).split() for item in infos][1]\n",
        "\n",
        "  language = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Idioma'][0]\n",
        "\n",
        "  n_pages = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Páginas'][0]\n",
        "\n",
        "  publishing = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Editora'][0]\n",
        "\n",
        "  binding = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Acabamento'][0]\n",
        "\n",
        "  isbn = [infos[i+1] for i in range(len(infos)) if infos[i] == 'ISBN 13' or infos[i] == 'ISBN' or infos[i] == 'ISBN-13'][0]\n",
        "          \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2gZWfJsLdOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in extra_urls:\n",
        "  frame = extra_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_ext = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLGIQli1-fzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_ext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAcv90pUjF_Q",
        "colab_type": "text"
      },
      "source": [
        "##7. CULTURA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rGXyR7cvsq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cultura_single(site):\n",
        "  soup = bs(open(site),\"html.parser\")\n",
        "\n",
        "  title = soup.find('h1', {'class':'title'}).text\n",
        "\n",
        "  author = soup.find('ul', {'class':'info'}).a.text\n",
        "\n",
        "  infos = soup.find('ul', {'id':'product-list-detail'}).find_all('li')\n",
        "  infos = [(re.sub('\\n|<.*?>|\\\\n|:', '', str(item))).split() for item in infos] \n",
        "\n",
        "  isbn = [info[i+1] for info in infos  for i in range(len(info)) if info[i] == 'ISBN'][0]\n",
        "\n",
        "  language = [info[i+1] for info in infos  for i in range(len(info)) if info[i] == 'Idioma'][0]\n",
        "\n",
        "  n_pages = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Páginas'][0]\n",
        "\n",
        "  publishing = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Editora'][0]\n",
        "\n",
        "  binding = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Encadernação'][1]\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "widj_x40xFvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in cultura_urls:\n",
        "  frame = cultura_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_cul = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF4ZznNM-H3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_cul"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aajCiOfZxuK_",
        "colab_type": "text"
      },
      "source": [
        "## 8. DISAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HXE9p5kBqdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def disal_single(site):\n",
        "  soup = bs(open(site), 'html.parser')\n",
        "\n",
        "  info_container = soup.find('div', {'class':'book-title'}).contents\n",
        "  info_container = [re.sub('\\n|<.*?>', '', str(info)) for info in info_container]\n",
        "  info_container = [info.strip() for info in info_container if info != '']\n",
        "\n",
        "  author = info_container[1]\n",
        "\n",
        "  title = info_container[0]\n",
        "\n",
        "  publishing = info_container[2]\n",
        "\n",
        "  infos = soup.find('table', {'class':'table table-striped'}).find_all('tr')\n",
        "  infos = [(re.sub('\\n|<.*?>|\\\\n', '', str(item))).split(':') for item in infos]\n",
        "\n",
        "  isbn = [info[i+1] for info in infos  for i in range(len(info)) if info[i] == 'ISBN' or info[i] == 'ISBN 13'][0]\n",
        "\n",
        "  language = [info[i+1] for info in infos  for i in range(len(info)) if info[i] == 'Idioma' or info[i] == 'Idioma '][0]\n",
        "\n",
        "  n_pages = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Páginas' or info[i] == 'Número de páginas'][0]\n",
        "\n",
        "  binding = [info[i+1] for info in infos for i in range(len(info)) if info[i] == 'Encadernação' or info[i] == 'Tipo de produto '][0]\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hE9vujSCsVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in disal_urls:\n",
        "  frame = disal_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_dis = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGsFSfg997Qu",
        "colab_type": "code",
        "outputId": "1b77db98-c7eb-4e6e-d7fa-ccfcff17fa7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dfs_dis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Título</th>\n",
              "      <th>Autor</th>\n",
              "      <th>Nº Páginas</th>\n",
              "      <th>Editora</th>\n",
              "      <th>Idioma</th>\n",
              "      <th>Acabamento</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Documento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ambiente E Os Processos De Maturacao, O</td>\n",
              "      <td>Winnicott, Dona</td>\n",
              "      <td>268</td>\n",
              "      <td>Artmed</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788573074567</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Autorretrato E Outras Crônicas</td>\n",
              "      <td>Andrade, Drummond De Andrade</td>\n",
              "      <td>256</td>\n",
              "      <td>Record</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788501112330</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aventuras Na Netoland Com Luccas Neto</td>\n",
              "      <td>Neto, Luccas</td>\n",
              "      <td>64</td>\n",
              "      <td>Pixel Media (nova Fronteira)</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788555461002</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exames Laboratoriais E Diagnosticos Em Enferma...</td>\n",
              "      <td>Fischbach, Frances</td>\n",
              "      <td>732</td>\n",
              "      <td>Guanabara (grupo Gen)</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788527726634</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fisiologia Endocrina (lange) - 4º Ed</td>\n",
              "      <td>Molina, Patricia E.</td>\n",
              "      <td>309</td>\n",
              "      <td>Mcgraw Hill (artmed)</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788580553918</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gestao Hospitalar - Para Uma Administracao Eficaz</td>\n",
              "      <td>Malagon-londono, Gustavo</td>\n",
              "      <td>612</td>\n",
              "      <td>Guanabara (grupo Gen)</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788527733298</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Gramatica Da Lingua Portuguesa Para Leigos</td>\n",
              "      <td>Schlee, Magda Bahia</td>\n",
              "      <td>248</td>\n",
              "      <td>Alta Books</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788550803395</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Introducao A Analise Matematica</td>\n",
              "      <td>Avila, Geraldo</td>\n",
              "      <td>260</td>\n",
              "      <td>Edgard Blucher</td>\n",
              "      <td>Português</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788521201687</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Michaelis Minidicionario Alemao - Alemao-portu...</td>\n",
              "      <td>Melhoramentos</td>\n",
              "      <td>480</td>\n",
              "      <td>Melhoramentos</td>\n",
              "      <td>Alemão-português / português-alemão</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788506078563</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Michaelis Minidicionario Espanhol - Espanhol-p...</td>\n",
              "      <td>Melhoramentos</td>\n",
              "      <td>462</td>\n",
              "      <td>Melhoramentos</td>\n",
              "      <td>Português - Espanhol</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788506078532</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Michaelis Minidicionario Frances - Frances-por...</td>\n",
              "      <td>Melhoramentos</td>\n",
              "      <td>510</td>\n",
              "      <td>Melhoramentos</td>\n",
              "      <td>Português - Francês</td>\n",
              "      <td>Livro</td>\n",
              "      <td>9788506078549</td>\n",
              "      <td>/content/drive/My Drive/9periodo/Recuperação...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Título  ...                                          Documento\n",
              "0             Ambiente E Os Processos De Maturacao, O  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "1                      Autorretrato E Outras Crônicas  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "2               Aventuras Na Netoland Com Luccas Neto  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "3   Exames Laboratoriais E Diagnosticos Em Enferma...  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "4                Fisiologia Endocrina (lange) - 4º Ed  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "5   Gestao Hospitalar - Para Uma Administracao Eficaz  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "6          Gramatica Da Lingua Portuguesa Para Leigos  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "7                     Introducao A Analise Matematica  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "8   Michaelis Minidicionario Alemao - Alemao-portu...  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "9   Michaelis Minidicionario Espanhol - Espanhol-p...  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "10  Michaelis Minidicionario Frances - Frances-por...  ...  /content/drive/My Drive/9periodo/Recuperação...\n",
              "\n",
              "[11 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iy6J1ofDwH7",
        "colab_type": "text"
      },
      "source": [
        "##9. AMERICANAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEKnYAvQnWZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def americanas_single(site):\n",
        "  soup = bs(open(site), 'html.parser')\n",
        "\n",
        "  infos = soup.find_all('table')\n",
        "  infos = [info.find_all('td') for info in infos]\n",
        "  infos = [info.text for info in infos[0]]\n",
        "\n",
        "  isbn = [infos[i+1] for i in range(len(infos)) if infos[i] == 'ISBN' or infos[i] == 'ISBN 13' or infos[i] == 'ISBN-13']\n",
        "\n",
        "  language = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Idioma' or infos[i] == 'Idioma ']\n",
        "\n",
        "  n_pages = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Páginas' or infos[i] == 'Número de páginas']\n",
        "\n",
        "  binding = [infos[i+1] for info in infos for i in range(len(infos)) if infos[i] == 'Encadernação' or infos[i] == 'Tipo de produto ' or infos[i] == 'Formato']\n",
        "\n",
        "  title = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Título']\n",
        "\n",
        "  author = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Autor']\n",
        "\n",
        "  publishing = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Editora']\n",
        "\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jvw348V_9Mi_",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in americanas_urls:\n",
        "  frame = americanas_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_ame = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sad3-qiK9eXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_ame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beUSCxJurEKj",
        "colab_type": "text"
      },
      "source": [
        "## 10. SUBMARINO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuHkP26vsPSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def submarino_single(site):\n",
        "  soup = bs(open(site), 'html.parser')\n",
        "\n",
        "  infos = soup.find_all('table')\n",
        "  infos = [info.find_all('td') for info in infos]\n",
        "  infos = [info.text for info in infos[0]]\n",
        "\n",
        "  isbn = [infos[i+1] for i in range(len(infos)) if infos[i] == 'ISBN' or infos[i] == 'ISBN 13' or infos[i] == 'ISBN-13']\n",
        "\n",
        "  language = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Idioma' or infos[i] == 'Idioma ']\n",
        "\n",
        "  n_pages = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Páginas' or infos[i] == 'Número de páginas']\n",
        "\n",
        "  binding = [infos[i+1] for info in infos for i in range(len(infos)) if infos[i] == 'Encadernação' or infos[i] == 'Tipo de produto ' or infos[i] == 'Formato']\n",
        "\n",
        "  title = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Título']\n",
        "\n",
        "  author = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Autor']\n",
        "\n",
        "  publishing = [infos[i+1] for i in range(len(infos)) if infos[i] == 'Editora']\n",
        "\n",
        "    \n",
        "  features = [title, author, n_pages, publishing, language, binding, isbn, site]\n",
        "  \n",
        "  feats = []\n",
        "  \n",
        "  for subfeature in features:\n",
        "    if len(subfeature) == 0:\n",
        "      feats.append('')\n",
        "    elif isinstance(subfeature, str):\n",
        "      feats.append(subfeature)\n",
        "    elif len(subfeature) == 1:\n",
        "      feats.append(subfeature[0])\n",
        "    else:\n",
        "      feats.append(subfeature[0])\n",
        "\n",
        "  return create_df(feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV2hHYF44dZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = []\n",
        "\n",
        "for url in submarino_urls:\n",
        "  frame = submarino_single(url)\n",
        "  frames.append(frame)\n",
        "\n",
        "dfs_sub = pd.concat(frames,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIdADgqc5DLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfs_sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tBdkzE6eRcq",
        "colab_type": "text"
      },
      "source": [
        "# Índice Invertido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9tc8ZqrDhHR",
        "colab_type": "text"
      },
      "source": [
        "### Creating DFS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATjufCQXz9Ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pagesLocation = '/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/'\n",
        "\n",
        "symbolsToReplace = [\n",
        "    '(', ')', '!', '?', ':', ';', '*', '.', ',', '★', '|', '+', '[', ']', '{', '}', '/', 'ª', 'º', '°', '-',\n",
        "    '%', '—', '@', '#',  '\\\"','\\'', '<', '>', '=', '´', '`', '“', '$', '&', '’', '¡', '€', 'µ', '¦',\n",
        "    '\\\\', '®', '™', '”', '…', '‘', '•', '😍','😙', '❤', '–', '⭐️', '_', '️⭐', '😉', '👏', '¹', '²', '³',\n",
        "    \"£\", '¢', '¬', '§', '~', '^', '×', '÷',\n",
        "    '\\u200c', '\\u200f', '\\u200e', '\\t', '\\xa0', '\\x03', '\\u0301', '\\u0303'\n",
        "]\n",
        "\n",
        "allowedSmallTokens = [\n",
        "  'fio', 'box', 'god', 'key', 'bit', 'led', 'faq', 'dog', 'sim', 'ovo', 'pão', 'aba', 'new'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLY2LY2o0Js7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = []\n",
        "with open(pagesLocation + 'stopwords.txt', 'r') as f:\n",
        "    stopwords = [line.strip() for line in f]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SPcOIcf0M7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAndCleanHTML(filepath):\n",
        "  \n",
        "  page = open(filepath, \"rb\")\n",
        "\n",
        "  page = page.read()\n",
        "\n",
        "  clearPage = UnicodeDammit.detwingle(page)\n",
        "  \n",
        "  doc = bs(clearPage, 'lxml')\n",
        "  \n",
        "  for script in doc([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "  \n",
        "  docText = doc.get_text(' ')\n",
        "  \n",
        "  docText = docText.translate({ord(ch): None for ch in '0123456789'})\n",
        "  \n",
        "  docText = docText.lower()\n",
        "  \n",
        "  docText = docText.replace(\"e-book\", \"ebook\")\n",
        "\n",
        "  docText = docText.replace(\"blu-ray\", \"bluray\")\n",
        "  \n",
        "  for symbol in symbolsToReplace:\n",
        "    docText = docText.replace(symbol, ' ')\n",
        "  \n",
        "  docText = docText.replace(\"  \", ' ')\n",
        "  \n",
        "  lines = (line.strip() for line in docText.splitlines())\n",
        "  \n",
        "  chunks = (phrase.strip() for line in lines for phrase in line.split(' '))\n",
        "\n",
        "  text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "  \n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCX4YbCH0ZQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allPositiveTexts = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLNibe8C0bfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positiveFilesList = [f for f in glob.glob(pagesLocation + \"**/Positivas/*.html\", recursive=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv7A00GT0ebg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractText(filesList, allTexts):\n",
        "  for file in filesList:\n",
        "    cleanText = getAndCleanHTML(file)\n",
        "    allTexts.append(cleanText)\n",
        "\n",
        "extractText(positiveFilesList, allPositiveTexts)\n",
        "\n",
        "# print(len(positiveFilesList), len(allPositiveTexts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpN4cG82xa4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# allPositiveTexts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AhQ7BNM0pKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positiveVocab = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV4ACz6v1h6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textToTokenList(text):  \n",
        "  return text.split(' ')\n",
        "\n",
        "def removeStopwords(list):\n",
        "  return [word for word in list if word not in stopwords]\n",
        "\n",
        "def removeSmallTokens(list):\n",
        "  return [word for word in list if len(word)>3 or word in allowedSmallTokens ]\n",
        "\n",
        "def updateVocabulary(token, vocab):\n",
        "  if token in vocab:\n",
        "    vocab.update({token: vocab.get(token) + 1})\n",
        "  else:\n",
        "    vocab[token] = 1\n",
        "\n",
        "def buildUnorderedDict(allTexts, vocab):\n",
        "  for text in allTexts:\n",
        "    tokens = textToTokenList(text)\n",
        "    tokens = removeStopwords(tokens)\n",
        "    tokens = removeSmallTokens(tokens)\n",
        "    for token in tokens:\n",
        "      updateVocabulary(token, vocab)\n",
        "\n",
        "def orderDictToTuplesList(unorderedDict):\n",
        "  return sorted(unorderedDict.items(), key = lambda kv: kv[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEEdx-cv1sv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buildUnorderedDict(allPositiveTexts, positiveVocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i0_V4N912aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pluralToSingular(token):\n",
        "  length = len(token)\n",
        "  lastChar = token[length - 1]\n",
        "  \n",
        "  if lastChar != 's':\n",
        "    return token\n",
        "  \n",
        "  last2Chars = token[length - 2] + lastChar\n",
        "  last3Chars = token[length - 3] + last2Chars\n",
        "  \n",
        "  ## z   -> zes\n",
        "  ## r   -> res\n",
        "  ## m   -> ns ****\n",
        "  ##*al  -> *ais\n",
        "  ##*el  -> *eis ***\n",
        "  ##*ol  -> *ois\n",
        "  ##*ul  -> *uis\n",
        "  ##*il  -> *is\n",
        "  ##*s   -> *ses\n",
        "  \n",
        "  ##*ões <- ão\n",
        "  ##*ães <- ão\n",
        "  ##*ões <- ão\n",
        "  \n",
        "  singular = token\n",
        "  \n",
        "  if last3Chars == 'uis':\n",
        "    #ul\n",
        "    return token[0 : length - 2] + 'l'\n",
        "  \n",
        "  elif last3Chars == 'éis':\n",
        "    #el\n",
        "    return token[0 : length - 3] + 'el'\n",
        "  \n",
        "  elif last3Chars == 'óis':\n",
        "    #ol\n",
        "    return token[0 : length - 3] + 'ol'\n",
        "    \n",
        "  elif last3Chars == 'zes' or last3Chars == 'res' or last3Chars == 'ses':\n",
        "    #z r s\n",
        "    return token[0 : length - 2]\n",
        "    \n",
        "  elif last3Chars == 'ãos' or last3Chars == 'ães' or last3Chars == 'ões':\n",
        "    #ão\n",
        "    return token[0 : length - 3] + 'ão'\n",
        "    \n",
        "  elif last3Chars == 'ais' or last2Chars == 'ns' or last3Chars == 'eis' or last3Chars == 'ois' or last2Chars == 'ís':\n",
        "    #ignorar\n",
        "    return singular\n",
        "  \n",
        "  elif last2Chars == 'is':\n",
        "    #il\n",
        "    return token[0 : length - 1] + 'l'\n",
        "  \n",
        "  else:\n",
        "    #tira o s\n",
        "    return token[0 : length - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmnQvGYi2LPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pluralOnlyStemming(vocab):\n",
        "  d = vocab.copy()\n",
        "  tokens = vocab.keys()\n",
        "  \n",
        "  for token in tokens:\n",
        "    singular = pluralToSingular(token)\n",
        "    if token != singular and singular in tokens:\n",
        "      \"\"\"print(\"Token: \" + token)\n",
        "      print(\"Singular: \" + singular)\"\"\"\n",
        "      d[singular] = d[singular] + d[token]\n",
        "      ##print(\"Merged: \" + singular + \" + \"  + token)\n",
        "      d.pop(token)\n",
        "      ##print(\"Popped: \" + token)\n",
        "      \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zII0yd8E2Nz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmedPV = pluralOnlyStemming(positiveVocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afJSUx2U2R6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(stemmedPV)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgLf1TeC2UXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## elimina palavras que aparecem menos de 5 vezes\n",
        "def removeLeastOccurringTokens(vocab):\n",
        "  newVocab = vocab.copy()\n",
        "  keys = vocab.keys()\n",
        "  \n",
        "  for key in keys:\n",
        "    if vocab[key] < 5:\n",
        "      newVocab.pop(key)\n",
        "  \n",
        "  return newVocab\n",
        "\n",
        "finalPV = removeLeastOccurringTokens(stemmedPV)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnhKRedc2fL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "orderedPositiveVocab = orderDictToTuplesList(finalPV)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btbp5uAh2jyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(orderedPositiveVocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvrU8wSf2m8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allTokens = set()\n",
        "allTokens = allTokens.union(finalPV.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uong25XH3i_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numDocs = len(positiveFilesList)\n",
        "numTokens = len(allTokens)\n",
        "\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=allTokens)\n",
        "dataframe['CLASS'] = np.zeros(shape=(numDocs, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW3mQsFQ3o9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(dataframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLom3tsw3rgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "everything = allPositiveTexts\n",
        "pLen = len(allPositiveTexts)\n",
        "isBook = 1\n",
        "\n",
        "def populateDataframe(isBook):\n",
        "  for i,row in dataframe.iterrows():\n",
        "    \n",
        "    if i==pLen:\n",
        "      isBook = 0\n",
        "      ##print(isBook)\n",
        "      \n",
        "    dataframe.at[i, 'CLASS'] = isBook\n",
        "    \n",
        "    tokens = textToTokenList(everything[i])\n",
        "    tokens = removeStopwords(tokens)\n",
        "    tokens = removeSmallTokens(tokens)\n",
        "    \n",
        "    for token in tokens:\n",
        "      s = pluralToSingular(token)\n",
        "      \n",
        "      if s in dataframe.columns:\n",
        "        dataframe.at[i, s] += 1\n",
        "        \n",
        "      elif token in dataframe.columns:\n",
        "        dataframe.at[i, token] += 1\n",
        "\n",
        "populateDataframe(isBook)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBBbK2dF33Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "timeNow = str(round(time.time() * 1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjkPKYTT39C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe.to_csv(pagesLocation + 'PaginasClassificadas' + timeNow + '.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2PhA-_N3-vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csvPath = pagesLocation + 'PaginasClassificadas' + timeNow + '.csv'\n",
        "csvData = np.genfromtxt(csvPath, delimiter=',', encoding='utf-8', dtype=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZx0xDUV4B5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numDocs2 = len(positiveFilesList)\n",
        "numTokens2 = len(allTokens)\n",
        "\n",
        "dados2 = np.zeros(shape=(numDocs2, numTokens2))\n",
        "dataframe2 = pd.DataFrame(data=dados2, columns=allTokens)\n",
        "dataframe2['CLASS'] = np.zeros(shape=(numDocs2, 1))\n",
        "\n",
        "\n",
        "# print(len(allPositiveTexts))\n",
        "everything2 = allPositiveTexts\n",
        "pLen2 = len(allPositiveTexts)\n",
        "isBook2 = 1\n",
        "\n",
        "def populateDataframeAbs(isBook2):\n",
        "  for i,row in dataframe2.iterrows():\n",
        "    \n",
        "    if i==pLen2:\n",
        "      isBook2 = 0\n",
        "      ##print(isBook)\n",
        "      \n",
        "    dataframe2.at[i, 'CLASS'] = isBook2\n",
        "    \n",
        "    tokens = textToTokenList(everything2[i])\n",
        "    tokens = removeStopwords(tokens)\n",
        "    tokens = removeSmallTokens(tokens)\n",
        "    \n",
        "    for token in tokens:\n",
        "      s = pluralToSingular(token)\n",
        "      \n",
        "      if s in dataframe2.columns:\n",
        "        dataframe2.at[i, s] = 1\n",
        "        \n",
        "      elif token in dataframe2.columns:\n",
        "        dataframe2.at[i, token] = 1\n",
        "\n",
        "populateDataframeAbs(isBook2)\n",
        "\n",
        "timeNow2 = str(round(time.time() * 1000))\n",
        "dataframe2.to_csv(pagesLocation + 'PaginasClassificadas_0e1_' + timeNow2 + '.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUDtmS05NfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfZeroOne = dataframe2.T\n",
        "# dfZeroOne.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2WryQcp69Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfFreqSort = dataframe.T.sort_index(axis=0, ascending=True).drop(['CLASS'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-HLMP5kZx3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfFreqSort.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUrei29K7TU7",
        "colab_type": "code",
        "outputId": "36f7f65a-cc45-4911-de1a-766778194a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "dfZeroOneSort = dfZeroOne.sort_index(axis=0, ascending=True).drop(['CLASS'])\n",
        "dfZeroOneSort.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abaixo</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aborda</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abordagem</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abra</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abraço</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 244 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0    1    2    3    4    5    6    ...  237  238  239  240  241  242  243\n",
              "abaixo     1.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "aborda     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "abordagem  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
              "abra       0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "abraço     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 244 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKDYubZYDpS8",
        "colab_type": "text"
      },
      "source": [
        "### Posting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SCqZziDzTb",
        "colab_type": "text"
      },
      "source": [
        "#### Compressed Posting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AESOycQbuXXa",
        "colab": {}
      },
      "source": [
        "def compr_posting (lst):\n",
        "  lst2 = [(lst[x] - lst[x-1]) for x in range(len(lst))]\n",
        "  lst2[0] = lst[0]\n",
        "  return(lst2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJWp6U-UD25H",
        "colab_type": "text"
      },
      "source": [
        "#### Posting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYWgRBNJgPHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_list (data_frame):\n",
        "  naozero = np.argwhere(data_frame.values != 0)\n",
        "\n",
        "  lista = [(x,y) for [x, y] in naozero]\n",
        "\n",
        "  nonz = pd.DataFrame(lista)\n",
        "\n",
        "  nonz.columns = ['word', 'doc']\n",
        "  nonz.doc = nonz.doc.astype(str)\n",
        "  nonz.doc += ','\n",
        "  non = nonz.groupby('word').doc.sum().str[:-1]\n",
        "\n",
        "  non_df = pd.DataFrame(non)\n",
        "\n",
        "  non_df['Terms'] = data_frame.index\n",
        "\n",
        "  basic_inv_index = non_df[['Terms', 'doc']]\n",
        "\n",
        "  basic_inv_index['to_list'] = basic_inv_index.loc[:, ('doc')].apply(lambda x: x.split(','))\n",
        "\n",
        "  basic_inv_index['Posting'] = basic_inv_index.loc[:, ('to_list')].apply(lambda x: list(map(int, x)))\n",
        "\n",
        "  basic_inv_index[\"Freq\"] = [len(basic_inv_index.loc[:, ('Posting')][x]) for x in range(len(basic_inv_index.loc[:, ('Posting')]))]\n",
        "\n",
        "  basic_inv_index = basic_inv_index.drop('to_list', axis=1)\n",
        "  basic_inv_index = basic_inv_index.drop('doc', axis=1) \n",
        "\n",
        "  basic_inv_index = basic_inv_index[['Terms', 'Freq', 'Posting']]\n",
        "\n",
        "  basic_inv_index['Compress Posting'] = basic_inv_index.apply(lambda x: compr_posting(x['Posting']), axis=1)\n",
        "\n",
        "  return (basic_inv_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnuXIa_UpyVy",
        "colab_type": "code",
        "outputId": "8ee8c88e-8128-47b1-d100-45f1674bb77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "indice_invertido = (to_list (dfFreqSort))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoB9jcG2fFQd",
        "colab_type": "code",
        "outputId": "d7e12607-ac12-4456-8c35-9005bf475942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "indice_invertido.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms</th>\n",
              "      <th>Freq</th>\n",
              "      <th>Posting</th>\n",
              "      <th>Compress Posting</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abaixo</td>\n",
              "      <td>69</td>\n",
              "      <td>[0, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 16, 17...</td>\n",
              "      <td>[0, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aborda</td>\n",
              "      <td>6</td>\n",
              "      <td>[14, 20, 29, 60, 96, 206]</td>\n",
              "      <td>[14, 6, 9, 31, 36, 110]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abordagem</td>\n",
              "      <td>10</td>\n",
              "      <td>[6, 13, 58, 158, 159, 165, 167, 209, 228, 240]</td>\n",
              "      <td>[6, 7, 45, 100, 1, 6, 2, 42, 19, 12]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abra</td>\n",
              "      <td>3</td>\n",
              "      <td>[87, 118, 164]</td>\n",
              "      <td>[87, 31, 46]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abraço</td>\n",
              "      <td>3</td>\n",
              "      <td>[87, 89, 93]</td>\n",
              "      <td>[87, 2, 4]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Terms  ...                                   Compress Posting\n",
              "word             ...                                                   \n",
              "0        abaixo  ...  [0, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, ...\n",
              "1        aborda  ...                            [14, 6, 9, 31, 36, 110]\n",
              "2     abordagem  ...               [6, 7, 45, 100, 1, 6, 2, 42, 19, 12]\n",
              "3          abra  ...                                       [87, 31, 46]\n",
              "4        abraço  ...                                         [87, 2, 4]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVnwnXcOfgzR",
        "colab_type": "text"
      },
      "source": [
        "### Lista de documentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjLddeuaVeCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pagesLocation2 = '/content/drive/My Drive/9periodo/Recuperação de Informação/SITES BAIXADOS (Cris)/'\n",
        "positiveFilesList2 = [f for f in glob.glob(pagesLocation + \"**/Positivas/*.html\", recursive=True)]\n",
        "positiveFilesList2_pd = pd.DataFrame(positiveFilesList2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylocAc5IXNdp",
        "colab_type": "text"
      },
      "source": [
        "### Índices das colunas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFd_DxTs1TB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_dfs = [dfs_ama, dfs_sar, dfs_trav, dfs_traca, dfs_cia, dfs_ext, dfs_cul, dfs_dis, dfs_ame, dfs_sub]\n",
        "dfs_tot = pd.concat(all_dfs,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7eA-CS2dT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs_tot.to_csv('dfs_tot.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9WhF0jC3C9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_text = dfs_tot['Título'].tolist()\n",
        "author_text =  dfs_tot['Autor'].tolist()\n",
        "pages_text =  dfs_tot['Nº Páginas'].tolist()\n",
        "publishing_text =  dfs_tot['Editora'].tolist()\n",
        "language_text =  dfs_tot['Idioma'].tolist()\n",
        "binding_text =  dfs_tot['Acabamento'].tolist()\n",
        "isbn_text =  dfs_tot['ISBN'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idvUOM4_8DFP",
        "colab_type": "text"
      },
      "source": [
        "### Titulo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ycAdw2BikX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(lista):\n",
        "  clean_doc = []\n",
        "  for doc in lista:\n",
        "    \n",
        "    docText = doc.lower()\n",
        "    \n",
        "    docText = docText.replace(\"e-book\", \"ebook\")\n",
        "\n",
        "    docText = docText.replace(\"blu-ray\", \"bluray\")\n",
        "    \n",
        "    for symbol in symbolsToReplace:\n",
        "      docText = docText.replace(symbol, ' ')\n",
        "    \n",
        "    docText = docText.replace(\"  \", ' ')\n",
        "    \n",
        "    lines = (line.strip() for line in docText.splitlines())\n",
        "    \n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(' '))\n",
        "\n",
        "    text_tile = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    clean_doc.append(text_tile)\n",
        "    \n",
        "  return clean_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXE6Q5UrEZfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_text_clean = clean(title_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdahZh7_QpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_vocab = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G484m2hS8i-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buildUnorderedDict(title_text_clean, title_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1B8WmOB341jq",
        "colab": {}
      },
      "source": [
        "stemmed_title = pluralOnlyStemming(title_vocab)\n",
        "# print(len(stemmed_title),len(title_vocab))\n",
        "# print(stemmed_title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9DraREkABG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ordered_title_vocab = orderDictToTuplesList(stemmed_title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aud8VR4LAnE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_title_tokens = set()\n",
        "all_title_tokens = all_title_tokens.union(stemmed_title.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4H_9osKBI-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numDocs = len(urls)\n",
        "numTokens = len(all_title_tokens)\n",
        "\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=all_title_tokens)\n",
        "# dataframe['CLASS'] = np.zeros(shape=(numDocs, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNheDPZwSAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "everything = title_text_clean\n",
        "pLen = len(title_text_clean)\n",
        "isBook = 1\n",
        "\n",
        "populateDataframe(isBook)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nUJCYtwBTBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sort_title = dataframe.T.drop(['CLASS']).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzKrHJkjKBzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort_title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n1QftZlH3Ft",
        "colab_type": "code",
        "outputId": "3a903a14-ec26-4114-eea7-a8a40ed31be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "indice_invertido_title = (to_list (sort_title))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rg9eUk5NBpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = indice_invertido_title['Terms'].map(str) + '.title' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TtH8fYoNZnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indice_invertido_title['Field Text'] = df1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgXZX9b-H90T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# indice_invertido_title.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7bHLQnRN8kI",
        "colab_type": "text"
      },
      "source": [
        "###Autor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRtPaltsWzIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clean = clean(author_text)\n",
        "\n",
        "vocab = dict()\n",
        "buildUnorderedDict(text_clean, vocab)\n",
        "\n",
        "stemmed = pluralOnlyStemming(vocab)\n",
        "ordered_vocab = orderDictToTuplesList(stemmed)\n",
        "\n",
        "all_tokens = set()\n",
        "all_tokens = all_tokens.union(stemmed.keys())\n",
        "\n",
        "numDocs = len(urls)\n",
        "numTokens = len(all_tokens)\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=all_tokens)\n",
        "\n",
        "everything = text_clean\n",
        "pLen = len(text_clean)\n",
        "isBook = 1\n",
        "\n",
        "populateDataframe(isBook)\n",
        "\n",
        "author_sort_df = dataframe.T.drop(['CLASS']).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-whMJgOBSF0U",
        "colab_type": "code",
        "outputId": "44535dd7-8fc5-4164-84f4-56981f1cda3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "indice_invertido_author = (to_list (author_sort_df))\n",
        "df1 = indice_invertido_author['Terms'].map(str) + '.author'\n",
        "indice_invertido_author['Field Text'] = df1 \n",
        "# indice_invertido_author.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hafnYeAVaFaY",
        "colab_type": "text"
      },
      "source": [
        "###Editora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LThvDduHaIPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clean = clean(publishing_text)\n",
        "\n",
        "vocab = dict()\n",
        "buildUnorderedDict(text_clean, vocab)\n",
        "\n",
        "stemmed = pluralOnlyStemming(vocab)\n",
        "ordered_vocab = orderDictToTuplesList(stemmed)\n",
        "\n",
        "all_tokens = set()\n",
        "all_tokens = all_tokens.union(stemmed.keys())\n",
        "\n",
        "numDocs = len(urls)\n",
        "numTokens = len(all_tokens)\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=all_tokens)\n",
        "\n",
        "everything = text_clean\n",
        "pLen = len(text_clean)\n",
        "isBook = 1\n",
        "\n",
        "populateDataframe(isBook)\n",
        "\n",
        "publishing_sort_df = dataframe.T.drop(['CLASS']).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM4yRs49aMFB",
        "colab_type": "code",
        "outputId": "ad3aa3da-4c92-4c3a-f694-3aa2ab3c84f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "indice_invertido_publishing = (to_list (publishing_sort_df))\n",
        "df1 = indice_invertido_publishing['Terms'].map(str) + '.publishing'\n",
        "indice_invertido_publishing['Field Text'] = df1 \n",
        "# indice_invertido_publishing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EtIH1Xaa57q",
        "colab_type": "text"
      },
      "source": [
        "### Idioma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVX0c3GmaaOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clean = clean(language_text)\n",
        "\n",
        "vocab = dict()\n",
        "buildUnorderedDict(text_clean, vocab)\n",
        "\n",
        "stemmed = pluralOnlyStemming(vocab)\n",
        "ordered_vocab = orderDictToTuplesList(stemmed)\n",
        "\n",
        "all_tokens = set()\n",
        "all_tokens = all_tokens.union(stemmed.keys())\n",
        "\n",
        "numDocs = len(urls)\n",
        "numTokens = len(all_tokens)\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=all_tokens)\n",
        "\n",
        "everything = text_clean\n",
        "pLen = len(text_clean)\n",
        "isBook = 1\n",
        "\n",
        "populateDataframe(isBook)\n",
        "\n",
        "language_sort_df = dataframe.T.drop(['CLASS']).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2A355nbDkq",
        "colab_type": "code",
        "outputId": "1cd90779-3a32-4b3a-ef19-bea9300029f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "indice_invertido_language = (to_list (language_sort_df))\n",
        "df1 = indice_invertido_language['Terms'].map(str) + '.language'\n",
        "indice_invertido_language['Field Text'] = df1 \n",
        "# indice_invertido_language"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4ytT4nablzE",
        "colab_type": "text"
      },
      "source": [
        "### ISBN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQNTVPZ_bm-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clean = clean(isbn_text)\n",
        "\n",
        "vocab = dict()\n",
        "buildUnorderedDict(text_clean, vocab)\n",
        "\n",
        "stemmed = pluralOnlyStemming(vocab)\n",
        "ordered_vocab = orderDictToTuplesList(stemmed)\n",
        "\n",
        "all_tokens = set()\n",
        "all_tokens = all_tokens.union(stemmed.keys())\n",
        "\n",
        "numDocs = len(urls)\n",
        "numTokens = len(all_tokens)\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=all_tokens)\n",
        "\n",
        "everything = text_clean\n",
        "pLen = len(text_clean)\n",
        "isBook = 1\n",
        "\n",
        "populateDataframe(isBook)\n",
        "\n",
        "isbn_sort_df = dataframe.T.drop(['CLASS']).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4dauQ_3bt6q",
        "colab_type": "code",
        "outputId": "61e4bf37-f8a7-4265-bce0-9513d301dc31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "indice_invertido_isbn = (to_list (isbn_sort_df))\n",
        "df1 = indice_invertido_isbn['Terms'].map(str) + '.isbn'\n",
        "indice_invertido_isbn['Field Text'] = df1 \n",
        "# indice_invertido_isbn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGEBjNUlI8IE",
        "colab_type": "text"
      },
      "source": [
        "### Índice Invertido das páginas analisadas no P1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBLr3KoKJgRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_view_pages = []\n",
        "extractText(urls, all_view_pages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8TH8NEQJcHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = dict()\n",
        "buildUnorderedDict(all_view_pages, vocab)\n",
        "\n",
        "removeLeastOccurringTokens(vocab)\n",
        "\n",
        "stemmed = pluralOnlyStemming(vocab)\n",
        "\n",
        "stemmed = removeLeastOccurringTokens(stemmed)\n",
        "ordered_vocab = orderDictToTuplesList(stemmed)\n",
        "\n",
        "all_tokens = set()\n",
        "all_tokens = all_tokens.union(stemmed.keys())\n",
        "\n",
        "numDocs = len(urls)\n",
        "numTokens = len(all_tokens)\n",
        "dados = np.zeros(shape=(numDocs, numTokens))\n",
        "dataframe = pd.DataFrame(data=dados, columns=all_tokens)\n",
        "\n",
        "everything = all_view_pages\n",
        "pLen = len(all_view_pages)\n",
        "isBook = 1\n",
        "\n",
        "populateDataframe(isBook)\n",
        "\n",
        "all_view_pages_df = dataframe.T.drop(['CLASS']).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6HZGH4N4Hm",
        "colab_type": "code",
        "outputId": "f1f5f8a8-7f5f-4c0b-c8c8-fd6c28022eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "all_view_pages_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abaixo</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abordagem</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abrir</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abuso</th>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acabam</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>último</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>única</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>único</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>úteis</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>útil</th>\n",
              "      <td>16.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2714 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0     1    2     3     4     5    ...  96   97   98   99   100  101\n",
              "abaixo      1.0   1.0  0.0   1.0   1.0   1.0  ...  1.0  1.0  1.0  1.0  0.0  1.0\n",
              "abordagem   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "abrir       0.0   0.0  0.0   0.0   0.0   0.0  ...  8.0  8.0  8.0  8.0  8.0  8.0\n",
              "abuso       8.0   8.0  0.0   8.0   8.0   8.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "acabam      0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "...         ...   ...  ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...\n",
              "último      0.0   0.0  0.0   0.0   1.0   0.0  ...  1.0  2.0  0.0  2.0  1.0  1.0\n",
              "única       1.0   1.0  1.0   1.0   1.0   1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "único       0.0   0.0  0.0   0.0   0.0   1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "úteis       1.0   1.0  0.0   1.0   1.0   1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "útil       16.0  12.0  0.0  13.0  13.0  12.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[2714 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfbFkd7ZJeqn",
        "colab_type": "code",
        "outputId": "c86de340-6e2a-44b5-a691-1f206dd058e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "indice_invertido_all_view_pages = (to_list (all_view_pages_df))\n",
        "# df1 = indice_invertido_all_view_pages['Terms'].map(str) + '.isbn'\n",
        "# indice_invertido_all_view_pages['Field Text'] = df1 \n",
        "# indice_invertido_isbn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-87oRqRPSsM",
        "colab_type": "code",
        "outputId": "960aed14-e7e4-4477-f1cc-2e41576f0943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "indice_invertido_all_view_pages.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms</th>\n",
              "      <th>Freq</th>\n",
              "      <th>Posting</th>\n",
              "      <th>Compress Posting</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abaixo</td>\n",
              "      <td>38</td>\n",
              "      <td>[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "      <td>[0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>abordagem</td>\n",
              "      <td>3</td>\n",
              "      <td>[7, 64, 74]</td>\n",
              "      <td>[7, 57, 10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abrir</td>\n",
              "      <td>20</td>\n",
              "      <td>[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 9...</td>\n",
              "      <td>[82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abuso</td>\n",
              "      <td>8</td>\n",
              "      <td>[0, 1, 3, 4, 5, 6, 7, 8]</td>\n",
              "      <td>[0, 1, 2, 1, 1, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>acabam</td>\n",
              "      <td>6</td>\n",
              "      <td>[6, 49, 52, 55, 56, 57]</td>\n",
              "      <td>[6, 43, 3, 3, 1, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Terms  ...                                   Compress Posting\n",
              "word             ...                                                   \n",
              "0        abaixo  ...  [0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "1     abordagem  ...                                        [7, 57, 10]\n",
              "2         abrir  ...  [82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n",
              "3         abuso  ...                           [0, 1, 2, 1, 1, 1, 1, 1]\n",
              "4        acabam  ...                                [6, 43, 3, 3, 1, 1]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6cpLT2Du6Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indice_invertido_all_view_pages.to_csv('indice_invertido_all_view_pages.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRusxMKucQi5",
        "colab_type": "text"
      },
      "source": [
        "### Merged DFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfIfOCnzc4xM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged_df = pd.concat([indice_invertido_title, indice_invertido_author,indice_invertido_publishing,indice_invertido_language,indice_invertido_isbn])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6VLtpTYdDPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged_df2 = merged_df[['Field Text','Freq','Posting','Compress Posting']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNv07yQOdkm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged_df2 = merged_df2.sort_values(['Field Text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMr68FlIgepO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "field_texts = merged_df2.reset_index()\n",
        "# field_texts.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ArcWzUJqn-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "field_texts.to_csv('field_texts.csv', index=False)\n",
        "# files.download('positiveFilesList2_pd.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJhr1MXshhoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "urls_df = pd.DataFrame(urls)\n",
        "# urls_df.info()\n",
        "urls_df.to_csv('urls_df.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mfbEKv4Q49a",
        "colab_type": "code",
        "outputId": "1d0e1c73-8ff5-4219-d339-03a2ea35e629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "field_texts.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>Field Text</th>\n",
              "      <th>Freq</th>\n",
              "      <th>Posting</th>\n",
              "      <th>Compress Posting</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1808.title</td>\n",
              "      <td>1</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1900.title</td>\n",
              "      <td>1</td>\n",
              "      <td>[4]</td>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word  Field Text  Freq Posting Compress Posting\n",
              "0     0  1808.title     1     [0]              [0]\n",
              "1     1  1900.title     1     [4]              [4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnVgIrOggUBr",
        "colab_type": "text"
      },
      "source": [
        "### Gamma Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EoIhGmKqybM",
        "colab_type": "code",
        "outputId": "ca1ace62-6f24-4dba-afa4-b497460cb013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "y = bytearray([82])\n",
        "offset = y[1:]\n",
        "one = bytearray([1])\n",
        "zero = bytearray([0])\n",
        "one_con = bytearray()\n",
        "for i in range(len(offset)):\n",
        "  one_con += one\n",
        "ones = one_con + zero \n",
        "if len(offset) == 0: \n",
        "  con =  \n",
        "con = ones + y\n",
        "\n",
        "print('offset: ',offset,'\\ny: ', y,'\\nlen: ', len(offset),'\\none: ', one,'\\nones: ', ones,'\\ncon: ', con)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "offset:  bytearray(b'') \n",
            "y:  bytearray(b'R') \n",
            "len:  0 \n",
            "one:  bytearray(b'\\x01') \n",
            "ones:  bytearray(b'\\x00') \n",
            "con:  bytearray(b'\\x00R')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaj3W_-1RIqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gamma_code(num):\n",
        "  bin_n = int(bin(num)[2:])\n",
        "  offset = str(bin_n)[1:]\n",
        "  len_offset = len(offset)\n",
        "  ones_list = [1 for i in range(len_offset)]\n",
        "  unary = ''.join(map(str, ones_list))\n",
        "  length = unary + '0'\n",
        "  gamma = (length + offset)\n",
        "\n",
        "  return (gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-3a3G6Q6pK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posting_to_bin_fields = field_texts['Compress Posting'].tolist()\n",
        "bin_post_fields = [[gamma_code(i) for i in j] for j in posting_to_bin_fields]\n",
        "field_texts['Gamma Code'] = bin_post_fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UsOQBGkjy6A",
        "colab_type": "code",
        "outputId": "df5b0670-d842-43a9-924a-bcdd1dfcfd23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "field_texts.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>Field Text</th>\n",
              "      <th>Freq</th>\n",
              "      <th>Posting</th>\n",
              "      <th>Compress Posting</th>\n",
              "      <th>Gamma Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1808.title</td>\n",
              "      <td>1</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1900.title</td>\n",
              "      <td>1</td>\n",
              "      <td>[4]</td>\n",
              "      <td>[4]</td>\n",
              "      <td>[11000]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1967.publishing</td>\n",
              "      <td>1</td>\n",
              "      <td>[29]</td>\n",
              "      <td>[29]</td>\n",
              "      <td>[111101101]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1984.publishing</td>\n",
              "      <td>1</td>\n",
              "      <td>[32]</td>\n",
              "      <td>[32]</td>\n",
              "      <td>[11111000000]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>1987.publishing</td>\n",
              "      <td>1</td>\n",
              "      <td>[34]</td>\n",
              "      <td>[34]</td>\n",
              "      <td>[11111000010]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word       Field Text  Freq Posting Compress Posting     Gamma Code\n",
              "0     0       1808.title     1     [0]              [0]            [0]\n",
              "1     1       1900.title     1     [4]              [4]        [11000]\n",
              "2     0  1967.publishing     1    [29]             [29]    [111101101]\n",
              "3     1  1984.publishing     1    [32]             [32]  [11111000000]\n",
              "4     2  1987.publishing     1    [34]             [34]  [11111000010]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPeRCnH5kG12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posting_to_bin = indice_invertido['Compress Posting'].tolist()\n",
        "bin_post = [[gamma_code(i) for i in j] for j in posting_to_bin]\n",
        "indice_invertido['Gamma Code'] = bin_post"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJYqDTJlj1bu",
        "colab_type": "code",
        "outputId": "b6ba60af-e247-4450-c943-5ca409d49f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "indice_invertido.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms</th>\n",
              "      <th>Freq</th>\n",
              "      <th>Posting</th>\n",
              "      <th>Compress Posting</th>\n",
              "      <th>Gamma Code</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abaixo</td>\n",
              "      <td>69</td>\n",
              "      <td>[0, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 16, 17...</td>\n",
              "      <td>[0, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, ...</td>\n",
              "      <td>[0, 100, 0, 0, 0, 0, 0, 0, 0, 101, 0, 0, 100, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aborda</td>\n",
              "      <td>6</td>\n",
              "      <td>[14, 20, 29, 60, 96, 206]</td>\n",
              "      <td>[14, 6, 9, 31, 36, 110]</td>\n",
              "      <td>[1110110, 11010, 1110001, 111101111, 111110001...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abordagem</td>\n",
              "      <td>10</td>\n",
              "      <td>[6, 13, 58, 158, 159, 165, 167, 209, 228, 240]</td>\n",
              "      <td>[6, 7, 45, 100, 1, 6, 2, 42, 19, 12]</td>\n",
              "      <td>[11010, 11011, 11111001101, 1111110100100, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abra</td>\n",
              "      <td>3</td>\n",
              "      <td>[87, 118, 164]</td>\n",
              "      <td>[87, 31, 46]</td>\n",
              "      <td>[1111110010111, 111101111, 11111001110]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abraço</td>\n",
              "      <td>3</td>\n",
              "      <td>[87, 89, 93]</td>\n",
              "      <td>[87, 2, 4]</td>\n",
              "      <td>[1111110010111, 100, 11000]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Terms  ...                                         Gamma Code\n",
              "word             ...                                                   \n",
              "0        abaixo  ...  [0, 100, 0, 0, 0, 0, 0, 0, 0, 101, 0, 0, 100, ...\n",
              "1        aborda  ...  [1110110, 11010, 1110001, 111101111, 111110001...\n",
              "2     abordagem  ...  [11010, 11011, 11111001101, 1111110100100, 0, ...\n",
              "3          abra  ...            [1111110010111, 111101111, 11111001110]\n",
              "4        abraço  ...                        [1111110010111, 100, 11000]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVZ1H3OWPfDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posting_to_bin_pages_an = indice_invertido_all_view_pages['Compress Posting'].tolist()\n",
        "bin_post = [[gamma_code(i) for i in j] for j in posting_to_bin_pages_an]\n",
        "indice_invertido_all_view_pages['Gamma Code'] = bin_post"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjDA5egIPwYs",
        "colab_type": "code",
        "outputId": "475b37b2-41e5-4848-f0ab-79396ac8637f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "indice_invertido_all_view_pages.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms</th>\n",
              "      <th>Freq</th>\n",
              "      <th>Posting</th>\n",
              "      <th>Compress Posting</th>\n",
              "      <th>Gamma Code</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abaixo</td>\n",
              "      <td>38</td>\n",
              "      <td>[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "      <td>[0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>abordagem</td>\n",
              "      <td>3</td>\n",
              "      <td>[7, 64, 74]</td>\n",
              "      <td>[7, 57, 10]</td>\n",
              "      <td>[11011, 11111011001, 1110010]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abrir</td>\n",
              "      <td>20</td>\n",
              "      <td>[82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 9...</td>\n",
              "      <td>[82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>[1111110010010, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abuso</td>\n",
              "      <td>8</td>\n",
              "      <td>[0, 1, 3, 4, 5, 6, 7, 8]</td>\n",
              "      <td>[0, 1, 2, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[0, 0, 100, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>acabam</td>\n",
              "      <td>6</td>\n",
              "      <td>[6, 49, 52, 55, 56, 57]</td>\n",
              "      <td>[6, 43, 3, 3, 1, 1]</td>\n",
              "      <td>[11010, 11111001011, 101, 101, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Terms  ...                                         Gamma Code\n",
              "word             ...                                                   \n",
              "0        abaixo  ...  [0, 0, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
              "1     abordagem  ...                      [11011, 11111011001, 1110010]\n",
              "2         abrir  ...  [1111110010010, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3         abuso  ...                         [0, 0, 100, 0, 0, 0, 0, 0]\n",
              "4        acabam  ...               [11010, 11111001011, 101, 101, 0, 0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7NZNswADZRE",
        "colab_type": "text"
      },
      "source": [
        "### Memory Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D0IJfhVlsYI9",
        "colab": {}
      },
      "source": [
        "space_ii = indice_invertido_all_view_pages.memory_usage(index=False,deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bu75JnvQU1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "space_iift = field_texts.memory_usage(index=False,deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOVm_D52smfZ",
        "colab_type": "code",
        "outputId": "8e21a840-9015-4b5c-bfbe-62a13d5bac6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# diff all texts\n",
        "diff_space_ii = space_ii['Posting'] - space_ii['Compress Posting']\n",
        "diff_space_ii"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4680"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9PWZIfGtDQi",
        "colab_type": "code",
        "outputId": "193e9482-7e54-4bab-bc1e-9c14ef8a084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# diff field text\n",
        "diff_space_iift = space_iift['Posting'] - space_iift['Compress Posting']\n",
        "diff_space_iift"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHr58w3MtPQT",
        "colab_type": "code",
        "outputId": "5b3ba83a-51d9-4926-cccc-207f7c20ea67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print('Memory usage ft: \\n\\n', space_iift,'\\n\\n','Memory usage all texts: \\n\\n', space_ii)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage ft: \n",
            "\n",
            " word                 5712\n",
            "Field Text          52452\n",
            "Freq                 5712\n",
            "Posting             58944\n",
            "Compress Posting    58128\n",
            "Gamma Code          58128\n",
            "dtype: int64 \n",
            "\n",
            " Memory usage all texts: \n",
            "\n",
            " Terms               185194\n",
            "Freq                 21712\n",
            "Posting             529152\n",
            "Compress Posting    533832\n",
            "Gamma Code          533832\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mQTk5m8waam",
        "colab_type": "code",
        "outputId": "92a78c15-36fa-40be-d346-43e4cc339e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "indice_invertido_all_view_pages.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2714 entries, 0 to 2713\n",
            "Data columns (total 5 columns):\n",
            "Terms               2714 non-null object\n",
            "Freq                2714 non-null int64\n",
            "Posting             2714 non-null object\n",
            "Compress Posting    2714 non-null object\n",
            "Gamma Code          2714 non-null object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 207.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2UK66Yf2wzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}